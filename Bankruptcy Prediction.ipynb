{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents  \n",
    "<ol>\n",
    "    <b><li>Importing libraries</li></b>\n",
    "    <li><b>Importing and organizing the data</b>\n",
    "        <ol>\n",
    "            <li>Convert the columns types for the features to float</li>\n",
    "            <li>Convert the class label types to int</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Data Analysis and Preprocessing</b>\n",
    "        <ol>\n",
    "            <li>Missing Data Analysis\n",
    "                <ol>\n",
    "                    <li>Generate Sparsity Matrix for the missing data</li>\n",
    "                    <li>Generate Heat Map for the missing data</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li>Data Imputation\n",
    "                <ol>\n",
    "                    <li>Mean Imputation</li>\n",
    "                    <li>K-NN</li>\n",
    "                    <li>EM</li>\n",
    "                    <li>MICE</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li>Dealing with imbalanced data\n",
    "                <ol>\n",
    "                    <li>Oversampling with SMOTE</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Data Modeling</b>\n",
    "        <ol>\n",
    "            <li>K-Fold Cross validation</li>\n",
    "            <li>Models\n",
    "                <ol>\n",
    "                    <li>Gaussian Naive Bayes classifier</li>\n",
    "                    <li>Logistic Regression classifier</li>\n",
    "                    <li>Decision Tree classifier</li>\n",
    "                    <li>Random Forest classifier</li>\n",
    "                    <li>Extreme Gradient Boosting classifier</li>\n",
    "                    <li>Balanced Bagging classifier</li>\n",
    "                </ol>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Model Analysis</b>\n",
    "        <ol>\n",
    "            <li>Model ranking</li>\n",
    "            <li>Balanced Bagging: Effect of varying number of estimators on the accuracy scores on different datasets</li>\n",
    "            <li>Balanced Bagging: Plotting effect of number of estimators on Accuracy</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>References</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# For loading .arff files\n",
    "from scipy.io import arff\n",
    "\n",
    "# To analyze the type of missing data\n",
    "import missingno as msno\n",
    "\n",
    "import fancyimpute\n",
    "import impyute as impy\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# classification models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing and organizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset Link:</b>The Dataset can be found at [Polish bankruptcy dataset](https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of **5 .arff data files** with the names **`1year`, `2year`, `3year`, `4year`, `5year`**.   \n",
    "Load the .arff files and convert them into pandas dataframes using the `load_dataframes` function.   \n",
    "Give them new columns headers using the function `set_new_headers`. The column labels for the features are like `X1`, `X2`, ... , `X64`. The class label is `Y`.   \n",
    "Print the first 5 rows of a dataframe, to see how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_arff_raw_data():\n",
    "    N=5\n",
    "    return [arff.loadarff('data/' + str(i+1) + 'year.arff') for i in range(N)]\n",
    "\n",
    "def load_dataframes():\n",
    "    return [pd.DataFrame(data_i_year[0]) for data_i_year in load_arff_raw_data()]\n",
    "\n",
    "def set_new_headers(dataframes):\n",
    "    cols = ['X' + str(i+1) for i in range(len(dataframes[0].columns)-1)]\n",
    "    cols.append('Y')\n",
    "    for df in dataframes:\n",
    "        df.columns = cols\n",
    "\n",
    "dataframes = load_dataframes()\n",
    "\n",
    "set_new_headers(dataframes)    \n",
    "\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.A Convert the columns types for the features to float\n",
    "The numeric data shown in the dataframe above is infact a python object. Let us convert all the numberic features for all the dataframes into float to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dtypes of all the columns (other than the class label columns) to float.\n",
    "def convert_columns_type_float(dfs):\n",
    "    for i in range(5):\n",
    "        index = 1\n",
    "        while(index<=63):\n",
    "            colname = dfs[i].columns[index]\n",
    "            col = getattr(dfs[i], colname)\n",
    "            dfs[i][colname] = col.astype(float)\n",
    "            index+=1\n",
    "            \n",
    "convert_columns_type_float(dataframes)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.B Convert the class label types to int\n",
    "If we look the class label `Y`, we notice that the values are shown either as `b'0'` or `b'1'`   \n",
    "They actually correspond to bankruptcy being false and true respectively.   \n",
    "It is convenient to convert them to binary integers 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class labels for all the dataframes are originally in object type. Convert them to int types\n",
    "def convert_class_label_type_int(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        col = getattr(dfs[i], 'Y')\n",
    "        dfs[i]['Y'] = col.astype(int)\n",
    "        \n",
    "convert_class_label_type_int(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A Missing Data Analysis\n",
    "Surely, there is missing data. Let us now see how much of it is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_nan_rows(dataframes, verbose=False):\n",
    "    clean_dataframes = [df.dropna(axis=0, how='any') for df in dataframes]\n",
    "    if verbose:\n",
    "        for i in range(len(dataframes)):\n",
    "            print(str(i+1)+'year:','Original Length=', len(dataframes[i]), '\\tCleaned Length=', len(clean_dataframes[i]), '\\tMissing Data=', len(dataframes[i])-len(clean_dataframes[i]))\n",
    "    return clean_dataframes\n",
    "\n",
    "nan_dropped_dataframes = drop_nan_rows(dataframes, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step shows us that there are a lot of rows in each of the 5 dataframes which have missing data in at least one of the features. In most of these dataframes, the missing-data-rows correspond to more than 50% of the entire data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A.a Generate Sparsity Matrix for the missing data\n",
    "Now that we have established that there is a lot of missing data, let us find out if the missing data has some correlation.   \n",
    "The `matrix` function from the `missingno` library helps us generate sparsity matrix, which shows us the gaps in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the sparsity matrix (figure) for all the dataframes\n",
    "def generate_sparsity_matrix(dfs):\n",
    "    for i in range(5):\n",
    "        missing_df_i = dfs[i].columns[dfs[i].isnull().any()].tolist()\n",
    "        msno.matrix(dfs[i][missing_df_i], figsize=(20,5))\n",
    "\n",
    "generate_sparsity_matrix(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots of sparsity for all the 5 dataframes, we could notice a lot of sparsity for the feature `X37` has the highest sparsity among all the features for all the dataframes. The feature `X21` is sparse for some, if not all, dataframes. Also, more or less, all the features have missing data samples.   \n",
    "\n",
    "From the above sparsity-plot, we could only know how sparse the data is, yet we don't know if the data missing-ness is correlated among any features, i.e., is the data missing completely at random? Or are there any features that are missing together? as a next step, let us find out if there is some correlation among the features.\n",
    "\n",
    "However, by now it is clear that simply dropping all the rows with missing values, or eliminating all the features which have missing values is not a good approach of dealing with the missing data, as it leads to tremendous data loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A.b Generate Heat Map for the missing data   \n",
    "Now, let us find out if there is some correlation among the missing features.    \n",
    "\n",
    "Using the `heatmap` function from `missingno` library, let us plot the heatmaps for all the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_heatmap(dfs):\n",
    "    for i in range(5):\n",
    "        missing_df_i = dfs[i].columns[dfs[i].isnull().any()].tolist()\n",
    "        msno.heatmap(dfs[i][missing_df_i], figsize=(20,20))\n",
    "        \n",
    "generate_heatmap(dataframes)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat maps above, for all the 5 dataframes, describe the degree of nullity relationship between different features.    The range of this nullity correlation is from -1 to 1 (-1 ≤ R ≤ 1).    \n",
    "Features with no missing value are excluded in the heatmap. If the nullity correlation is very close to zero (-0.05 < R < 0.05), no value will be displayed.    \n",
    "\n",
    "A perfect positive nullity correlation (R=1) indicates when the first feature and the second feature both have corresponding missing values.       \n",
    "\n",
    "A perfect negative nullity correlation (R=-1) means that one of the features is missing and the second is not missing.   \n",
    "\n",
    "The takeaway is that, in each dataframe, there are some features that are heavily correlated (R = 1 or -1) and also there are features that are not essentially correlated (R values close to 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B Data Imputation\n",
    "\n",
    "It is now established that we need to impute (fill in the gaps) the missing data, as dropping the missing rows or eliminating the missing features is not an option.   \n",
    "\n",
    "We would like to explore some of the widely used missing data imputation techniques.   \n",
    "<b>\n",
    "1. Mean Imputation (baseline method)\n",
    "2. k Nearest Neighbors (k-NN) Imputation\n",
    "3. Expectation-Maximization (EM) Imputation\n",
    "4. Multivariate Imputation using Chained Equations (MICE)\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B.a Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mean_imputation(dfs):\n",
    "    # Construct an imputer with strategy as 'mean', to mean-impute along the columns\n",
    "    imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "    mean_imputed_dfs = [pd.DataFrame(imputer.fit_transform(df)) for df in dfs]\n",
    "    for i in range(len(dfs)):\n",
    "        mean_imputed_dfs[i].columns = dfs[i].columns   \n",
    "    return mean_imputed_dfs\n",
    "\n",
    "mean_imputed_dataframes = perform_mean_imputation(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B.b k-Nearest Neighbors (k-NN) Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_knn_imputation(dfs):\n",
    "    knn_imputed_datasets = [fancyimpute.KNN(k=100,verbose=True).complete(dfs[i]) for i in range(len(dfs))]\n",
    "    return [pd.DataFrame(data=knn_imputed_datasets[i]) for i in range(len(dfs))]\n",
    "    \n",
    "knn_imputed_dataframes = perform_knn_imputation(dataframes)\n",
    "set_new_headers(knn_imputed_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B.c Expectation-Maximization (EM) Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_EM_imputation(dfs):\n",
    "    em_imputed_datasets = [impy.imputations.cs.em(dfs[i].values, loops=50, dtype='cont') for i in range(len(dfs))]\n",
    "    return [pd.DataFrame(data=em_imputed_datasets[i]) for i in range(len(dfs))]\n",
    "\n",
    "em_imputed_dataframes = perform_EM_imputation(dataframes)\n",
    "set_new_headers(em_imputed_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B.d MICE imputation (Multivariate Imputation using Chained Equation)\n",
    "Obtaining the completed features for all the 5 dataframes by doing MICE (Multiple Imputation from Chained Equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_MICE_imputation(dfs):\n",
    "    mice_imputed_datasets = [fancyimpute.MICE(verbose=False).complete(dfs[i]) for i in range(len(dfs))]\n",
    "    return [pd.DataFrame(data=mice_imputed_datasets[i]) for i in range(len(dfs))]\n",
    "    \n",
    "mice_imputed_dataframes = perform_MICE_imputation(dataframes)\n",
    "set_new_headers(mice_imputed_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above 4 steps, we have successfully created 4 differently imputed dataframes using: Mean, k-NN, EM and MICE techniques respectively.   \n",
    "\n",
    "Here below, we create a dictionary of all the imputed dataframes to re-use them in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataframes_dictionary = OrderedDict()\n",
    "imputed_dataframes_dictionary['Mean'] = mean_imputed_dataframes\n",
    "imputed_dataframes_dictionary['k-NN'] = knn_imputed_dataframes\n",
    "imputed_dataframes_dictionary['EM'] = em_imputed_dataframes\n",
    "imputed_dataframes_dictionary['MICE'] = mice_imputed_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C Dealing with imbalanced data   \n",
    "\n",
    "In the steps seen above, we have successfully dealt with the missing data. But we have not dealt with the class imbalance (if any) in the data. Simply put, Data Imbalance is a condition where the samples belonging to one or more 'majority' class labels of a labelled dataset heavily outnumber the sample belonging to the other 'minority' classes.   \n",
    "\n",
    "Data imbalance critically affects the modeling as the models won't have sufficient data belonging to minority classes to train on and this leads to biased models, ultimately leading to poor performance on test data.   \n",
    "\n",
    "Firstly, let us see if our data is imbalanced, and to what extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_imbalance(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        print('Dataset: '+str(i+1)+'year')\n",
    "        print(dfs[i].groupby('Y').size())\n",
    "        minority_percent = (dfs[i]['Y'].tolist().count(1) / len(dfs[i]['Y'].tolist()))*100\n",
    "        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n",
    "        print('-'*64)\n",
    "        \n",
    "check_data_imbalance(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the step above that there is a lot of data imbalance for our datasets, as indicated by the percentage of minority class (label `1`) samples among their datasets. With this huge magnitude of data imbalance, the models will not train wel if we leave them as is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C.a Oversampling with SMOTE (Synthetic Minority Over Sampling Technique)\n",
    "First, we Split the features and labels into separate dataframes for all the original dataframes. Then, we will perform the SMOTE oversampling from given dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframes_features_labels(dfs):\n",
    "    feature_dfs = [dfs[i].iloc[:,0:64] for i in range(len(dfs))]\n",
    "    label_dfs = [dfs[i].iloc[:,64] for i in range(len(dfs))]\n",
    "    return feature_dfs, label_dfs\n",
    "\n",
    "def oversample_data_SMOTE(dfs, verbose=False):\n",
    "    smote = SMOTE(ratio='auto' , random_state=42, k_neighbors=10)\n",
    "    #Split the features and labels for each dataframe\n",
    "    feature_dfs, label_dfs = split_dataframes_features_labels(dfs)\n",
    "    resampled_feature_arrays = []\n",
    "    resampled_label_arrays = []\n",
    "    for i in range(len(dfs)):\n",
    "        if verbose: print('Dataset: ' + str(i+1) + 'year:')\n",
    "        if verbose: print('Original dataset shape {}'.format(Counter(label_dfs[i])))\n",
    "        dfi_features_res, dfi_label_res = smote.fit_sample(feature_dfs[i], label_dfs[i])\n",
    "        if verbose: print('Resampled dataset shape {}\\n'.format(Counter(dfi_label_res)))   \n",
    "        resampled_feature_arrays.append(dfi_features_res)\n",
    "        resampled_label_arrays.append(dfi_label_res)        \n",
    "    return resampled_feature_arrays, resampled_label_arrays\n",
    "\n",
    "# Utility Function to convert the arrays of features and labels to pandas dataframes, and then join them. Also re-assign the columns headers.\n",
    "def restructure_arrays_to_dataframes(feature_arrays, label_arrays):\n",
    "    resampled_dfs = []\n",
    "    for i in range(len(feature_arrays)):\n",
    "        feature_df = pd.DataFrame(data=feature_arrays[i])\n",
    "        label_df = pd.DataFrame(data=label_arrays[i])\n",
    "        label_df.columns=['Y'] \n",
    "        resampled_dfs.append(feature_df.join(label_df))\n",
    "    set_new_headers(resampled_dfs)    \n",
    "    return resampled_dfs\n",
    "\n",
    "# Perform SMOTE oversampling on all the imputed dataframes, and return them in a dictionary.\n",
    "def perform_oversampling_on_imputed_dataframes(df_dict):\n",
    "    imputed_oversampled_dataframes_dictionary = OrderedDict()\n",
    "    for key,dfs in df_dict.items():\n",
    "        print('SMOTE Oversampling for ' + key + ' imputed dataframes\\n')\n",
    "        smote_feature_arrays, smote_label_arrays = oversample_data_SMOTE(dfs, verbose=True)\n",
    "        oversampled_dataframes = restructure_arrays_to_dataframes(smote_feature_arrays, smote_label_arrays)\n",
    "        imputed_oversampled_dataframes_dictionary[key] = oversampled_dataframes\n",
    "        print('-'*100)\n",
    "    return imputed_oversampled_dataframes_dictionary\n",
    "\n",
    "imputed_oversampled_dataframes_dictionary = perform_oversampling_on_imputed_dataframes(imputed_dataframes_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Modeling: Building Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kfold_cv_data(k, X, y, verbose=False):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    kf = KFold(n_splits=k, shuffle=False, random_state=42)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train.append(X[train_index])\n",
    "        y_train.append(y[train_index])\n",
    "        X_test.append(X[test_index])\n",
    "        y_test.append(y[test_index])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.a Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gnb_classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.b Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(penalty = 'l1', random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.c Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.d Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators = 5, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.e Extreme Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.f Balanced Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_classifier = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'), n_estimators = 5, bootstrap = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary of models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dictionary = OrderedDict()\n",
    "\n",
    "models_dictionary['Gaussian Naive Bayes'] = gnb_classifier\n",
    "models_dictionary['Logistic Regression'] = lr_classifier\n",
    "models_dictionary['Decision Tree'] = dt_classifier\n",
    "models_dictionary['Extreme Gradient Boosting'] = xgb_classifier\n",
    "models_dictionary['Random Forest'] = rf_classifier\n",
    "models_dictionary['Balanced Bagging'] = bb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_data_modeling(_models_, _imputers_, verbose=False, k_folds=5):\n",
    "    \n",
    "    # 7 Models\n",
    "    # 4 Imputers\n",
    "    # 5 datasets (for 5 years)\n",
    "    # 7 metrics, averaged over all the K-Folds\n",
    "    model_results = OrderedDict()\n",
    "    \n",
    "\n",
    "    for model_name, clf in _models_.items():\n",
    "        if verbose: print(\"-\"*120, \"\\n\", \"Model: \" + '\\033[1m' + model_name + '\\033[0m' + \" Classifier\")\n",
    "        imputer_results = OrderedDict()\n",
    "        \n",
    "        # Iterate over the different imputed_data mechanisms (Mean, k-NN, EM, MICE)\n",
    "        for imputer_name, dataframes_list in _imputers_.items():\n",
    "            if verbose: print('\\tImputer Technique: ' + '\\033[1m' + imputer_name + '\\033[0m')\n",
    "            \n",
    "            feature_dfs, label_dfs = split_dataframes_features_labels(dataframes_list)            \n",
    "            \n",
    "            year_results = OrderedDict()\n",
    "            \n",
    "            for df_index in range(len(dataframes_list)):\n",
    "                if verbose: print('\\t\\tDataset: ' + '\\033[1m' + str(df_index+1) + 'year' + '\\033[0m')\n",
    "                \n",
    "                # Calling the 'prepare_kfold_cv_data' returns lists of features and labels \n",
    "                # for train and test sets respectively.\n",
    "                # The number of items in the list is equal to k_folds\n",
    "                X_train_list, y_train_list, X_test_list, y_test_list = prepare_kfold_cv_data(k_folds, feature_dfs[df_index], label_dfs[df_index], verbose)\n",
    "                \n",
    "                metrics_results = OrderedDict()\n",
    "                accuracy_list = np.zeros([k_folds])\n",
    "                precision_list = np.zeros([k_folds,2])\n",
    "                recall_list = np.zeros([k_folds,2])\n",
    "                TN_list = np.zeros([k_folds])\n",
    "                FP_list = np.zeros([k_folds])\n",
    "                FN_list = np.zeros([k_folds])\n",
    "                TP_list = np.zeros([k_folds])                \n",
    "                \n",
    "                for k_index in range(k_folds):\n",
    "                    X_train = X_train_list[k_index]\n",
    "                    y_train = y_train_list[k_index]\n",
    "                    X_test = X_test_list[k_index]\n",
    "                    y_test = y_test_list[k_index]\n",
    "                    \n",
    "                    clf = clf.fit(X_train, y_train)\n",
    "                    y_test_predicted = clf.predict(X_test)\n",
    "                    \n",
    "                    _accuracy_ = accuracy_score(y_test, y_test_predicted, normalize=True)\n",
    "                    accuracy_list[k_index] = _accuracy_\n",
    "                    \n",
    "                    _recalls_ = recall_score(y_test, y_test_predicted, average=None)\n",
    "                    recall_list[k_index] = _recalls_\n",
    "                    \n",
    "                    _precisions_ = precision_score(y_test, y_test_predicted, average=None)\n",
    "                    precision_list[k_index] = _precisions_\n",
    "                    \n",
    "                    _confusion_matrix_ = confusion_matrix(y_test, y_test_predicted)\n",
    "                    TN_list[k_index] = _confusion_matrix_[0][0]\n",
    "                    FP_list[k_index] = _confusion_matrix_[0][1]\n",
    "                    FN_list[k_index] = _confusion_matrix_[1][0]\n",
    "                    TP_list[k_index] = _confusion_matrix_[1][1]\n",
    "                \n",
    "                metrics_results['Accuracy'] = np.mean(accuracy_list)\n",
    "                metrics_results['Precisions'] = np.mean(precision_list, axis=0)\n",
    "                metrics_results['Recalls'] = np.mean(recall_list, axis=0)\n",
    "                metrics_results['TN'] = np.mean(TN_list)\n",
    "                metrics_results['FP'] = np.mean(FP_list)\n",
    "                metrics_results['FN'] = np.mean(FN_list)\n",
    "                metrics_results['TP'] = np.mean(TP_list)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('\\t\\t\\tAccuracy:', metrics_results['Accuracy'])\n",
    "                    print('\\t\\t\\tPrecision:', metrics_results['Precisions'])\n",
    "                    print('\\t\\t\\tRecall:', metrics_results['Recalls'])\n",
    "                \n",
    "                year_results[str(df_index+1)+'year'] = metrics_results   \n",
    "                \n",
    "            imputer_results[imputer_name] = year_results\n",
    "            \n",
    "        model_results[model_name] = imputer_results  \n",
    "        \n",
    "    return model_results                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = perform_data_modeling(models_dictionary, imputed_oversampled_dataframes_dictionary, verbose=True, k_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ranking\n",
    "model -> imputer -> year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_model_ranking(models, imputers, results):\n",
    "    column_headers = ['-'] + list(imputers.keys())\n",
    "    rows = []\n",
    "    for model_name, model_details in results.items():\n",
    "        row = [model_name]\n",
    "        for imputer_name, imputer_details in model_details.items():\n",
    "            mean_accuracy = 0\n",
    "            for year, metrics in imputer_details.items():\n",
    "                mean_accuracy += metrics['Accuracy']\n",
    "            mean_accuracy = mean_accuracy/len(imputer_details)\n",
    "            row.append(mean_accuracy)\n",
    "        rows.append(row)\n",
    "    results_df = pd.DataFrame(data=rows, columns = column_headers)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_model_ranking(models_dictionary, imputed_oversampled_dataframes_dictionary, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Bagging: Effect of varying number of estimators on the accuracy scores on different datasets\n",
    "\n",
    "This list stores results of Balanced Bagging classifier obtained by running it for \n",
    "various values of number of estimators in the range of 1 to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_by_estimators = []\n",
    "for i in range(29):\n",
    "    models_dictionary['Balanced Bagging'] = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'), n_estimators = i+1, bootstrap = True)\n",
    "    results = perform_data_modeling(models_dictionary, imputed_oversampled_dataframes_dictionary, verbose=True, k_folds=5)\n",
    "    results_by_estimators.append(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Bagging: Plotting effect of number of estimators on Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_values = []\n",
    "year2_values = []\n",
    "year3_values = []\n",
    "year4_values = []\n",
    "year5_values = []\n",
    "\n",
    "\n",
    "def extract_actual_values_from_dict(curr_dict):\n",
    "    temp_dict = curr_dict['Balanced Bagging']\n",
    "    return temp_dict['Mean']\n",
    "\n",
    "for i in range(29):\n",
    "    curr_dict = results_by_estimators[i]\n",
    "    curr_result = extract_actual_values_from_dict(curr_dict)\n",
    "    \n",
    "        \n",
    "    year_1_result = curr_result['1year']\n",
    "    year_2_result = curr_result['2year']\n",
    "    year_3_result = curr_result['3year']\n",
    "    year_4_result = curr_result['4year']\n",
    "    year_5_result = curr_result['5year']\n",
    "    year1_values.append(year_1_result['Accuracy'])\n",
    "    year2_values.append(year_2_result['Accuracy'])\n",
    "    year3_values.append(year_3_result['Accuracy'])\n",
    "    year4_values.append(year_4_result['Accuracy'])\n",
    "    year5_values.append(year_5_result['Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of effect of number of estimators on Accuracy for Balanced Bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "estimators = [i+1 for i in range(29)] \n",
    "\n",
    "\n",
    "plt.plot(estimators, year1_values, '.b-')\n",
    "plt.plot(estimators, year2_values, '.r-')\n",
    "plt.plot(estimators, year3_values, '.y-')\n",
    "plt.plot(estimators, year4_values, '.g-')\n",
    "plt.plot(estimators, year5_values, '.m-') \n",
    "plt.xlabel(\"\\nNumber of estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"\\nEffect of varying number of estimators on the accuracy scores on different datasets\\n\")\n",
    "\n",
    "# display legend\n",
    "plt.plot(10, 0.93, '.b-', label='Year 1')\n",
    "plt.plot(10, 0.93, '.r-', label='Year 2')\n",
    "plt.plot(10, 0.93, '.y-', label='Year 3')\n",
    "plt.plot(10, 0.93, '.g-', label='Year 4')\n",
    "plt.plot(10, 0.93, '.m-', label='Year 5')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/numpy-1.14.0/reference/\n",
    "https://pandas.pydata.org/\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.arff.loadarff.html\n",
    "https://github.com/iskandr/fancyimpute\n",
    "https://pypi.org/project/impyute/\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "https://docs.python.org/3/library/collections.html\n",
    "http://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.ensemble.BalancedBaggingClassifier.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "https://docs.python.org/2/library/random.html\n",
    "http://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
